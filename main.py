import ast
from datasets import load_dataset, DatasetDict, Dataset
from huggingface_hub import login

# ==========================================
# üõë ORGANIZER SETTINGS
# ==========================================
HF_TOKEN = ""
login(token=HF_TOKEN)

HF_REPO_NAME = "mashriram/AI4Bharat-Languages-and-Cultures"

# ==========================================
# üîß PATCH SCRIPT ‚Äî fixes two issues:
#
# FIX 1 ‚Äî Andhra Pradesh conv wrong dataset name:
#   ‚ùå Telugu-LLM-Labs/telugu_alpaca_yahma_cleaned_filtered
#   ‚úÖ Telugu-LLM-Labs/telugu_alpaca_yahma_cleaned_filtered_romanized
#
# FIX 2 ‚Äî All wikiqa cult splits returned 0 rows:
#   microsoft/wiki_qa has almost no India-specific content.
#   Replaced with wiki_filtered: load the state's actual Wikipedia
#   dump and filter articles by native-language title keywords.
#   For NE states with no native wiki ‚Üí kept wiki_qa but with
#   proper column-level filtering (question + document_title).
#
# This script ONLY re-uploads the affected splits.
# It does NOT re-process indic or conv for unaffected states.
# ==========================================

# ==========================================
# üó∫Ô∏è PATCH CONFIG ‚Äî only states that need fixes
# ==========================================

# States that had wikiqa cult type ‚Äî now using wiki_filtered
# cult_val     = wikimedia/wikipedia dump ID
# cult_lang    = native script keywords to filter article titles
# cult_fallback= romanized/common keywords as backup if native filter yields 0

WIKIQA_PATCH = {
    "Telangana": {
        "cult_val": "20231101.te",
        "cult_keywords": ["‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£", "‡∞π‡±à‡∞¶‡∞∞‡∞æ‡∞¨‡∞æ‡∞¶‡±ç", "‡∞ö‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞®‡∞æ‡∞∞‡±ç", "‡∞®‡∞ø‡∞ú‡∞æ‡∞Ç"],
        "cult_fallback": ["Telangana", "Hyderabad", "Charminar"],
    },
    "Rajasthan": {
        "cult_val": "20231101.hi",
        "cult_keywords": ["‡§∞‡§æ‡§ú‡§∏‡•ç‡§•‡§æ‡§®", "‡§ú‡§Ø‡§™‡•Å‡§∞", "‡§•‡§æ‡§∞", "‡§∞‡§æ‡§ú‡§™‡•Ç‡§§", "‡§â‡§¶‡§Ø‡§™‡•Å‡§∞"],
        "cult_fallback": ["Rajasthan", "Jaipur", "Thar"],
    },
    "Haryana": {
        "cult_val": "20231101.hi",
        "cult_keywords": ["‡§π‡§∞‡§ø‡§Ø‡§æ‡§£‡§æ", "‡§ï‡•Å‡§∞‡•Å‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞", "‡§ó‡•Å‡§∞‡•Å‡§ó‡•ç‡§∞‡§æ‡§Æ", "‡§™‡§æ‡§®‡•Ä‡§™‡§§"],
        "cult_fallback": ["Haryana", "Kurukshetra"],
    },
    "Himachal_Pradesh": {
        "cult_val": "20231101.hi",
        "cult_keywords": ["‡§π‡§ø‡§Æ‡§æ‡§ö‡§≤ ‡§™‡•ç‡§∞‡§¶‡•á‡§∂", "‡§∂‡§ø‡§Æ‡§≤‡§æ", "‡§Æ‡§®‡§æ‡§≤‡•Ä", "‡§ß‡§∞‡•ç‡§Æ‡§∂‡§æ‡§≤‡§æ"],
        "cult_fallback": ["Himachal Pradesh", "Shimla", "Manali"],
    },
    "Uttar_Pradesh": {
        "cult_val": "20231101.hi",
        "cult_keywords": ["‡§â‡§§‡•ç‡§§‡§∞ ‡§™‡•ç‡§∞‡§¶‡•á‡§∂", "‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä", "‡§Ö‡§µ‡§ß", "‡§≤‡§ñ‡§®‡§ä", "‡§Ü‡§ó‡§∞‡§æ"],
        "cult_fallback": ["Uttar Pradesh", "Varanasi", "Lucknow", "Agra"],
    },
    "Madhya_Pradesh": {
        "cult_val": "20231101.hi",
        "cult_keywords": ["‡§Æ‡§ß‡•ç‡§Ø ‡§™‡•ç‡§∞‡§¶‡•á‡§∂", "‡§≠‡•ã‡§™‡§æ‡§≤", "‡§á‡§Ç‡§¶‡•å‡§∞", "‡§ó‡•ç‡§µ‡§æ‡§≤‡§ø‡§Ø‡§∞"],
        "cult_fallback": ["Madhya Pradesh", "Bhopal", "Indore"],
    },
    "Chhattisgarh": {
        "cult_val": "20231101.hi",
        "cult_keywords": ["‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º", "‡§∞‡§æ‡§Ø‡§™‡•Å‡§∞", "‡§¨‡§∏‡•ç‡§§‡§∞", "‡§¨‡§ø‡§≤‡§æ‡§∏‡§™‡•Å‡§∞"],
        "cult_fallback": ["Chhattisgarh", "Raipur", "Bastar"],
    },
    "Delhi": {
        "cult_val": "20231101.hi",
        "cult_keywords": ["‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä", "‡§®‡§à ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä", "‡§≤‡§æ‡§≤ ‡§ï‡§ø‡§≤‡§æ", "‡§Æ‡•Å‡§ó‡§≤"],
        "cult_fallback": ["Delhi", "New Delhi", "Red Fort", "Mughal"],
    },
    "Tripura": {
        "cult_val": "20231101.bn",  # Bengali Wikipedia ‚Äî widely spoken in Tripura
        "cult_keywords": ["‡¶§‡ßç‡¶∞‡¶ø‡¶™‡ßÅ‡¶∞‡¶æ", "‡¶Ü‡¶ó‡¶∞‡¶§‡¶≤‡¶æ", "‡¶ï‡ßã‡¶ï‡¶¨‡¶∞‡¶ï"],
        "cult_fallback": ["Tripura", "Agartala"],
    },
    # --- NE states with no strong native Wikipedia ---
    # Keep wiki_qa but with proper column-level filter
    "Arunachal_Pradesh": {
        "cult_val": None,  # no usable native wiki ‚Üí use wiki_qa fallback
        "cult_keywords": [],
        "cult_fallback": ["Arunachal Pradesh", "Tawang", "monastery"],
    },
    "Meghalaya": {
        "cult_val": None,
        "cult_keywords": [],
        "cult_fallback": ["Meghalaya", "Shillong", "Khasi", "Cherrapunji"],
    },
    "Mizoram": {
        "cult_val": None,
        "cult_keywords": [],
        "cult_fallback": ["Mizoram", "Aizawl", "Mizo"],
    },
    "Nagaland": {
        "cult_val": None,
        "cult_keywords": [],
        "cult_fallback": ["Nagaland", "Kohima", "Naga", "Hornbill"],
    },
}

# Andhra Pradesh ‚Äî only conv needs fixing, cult (wiki) was fine
ANDHRA_CONV_FIX = {
    "conv": "Telugu-LLM-Labs/telugu_alpaca_yahma_cleaned_filtered_romanized"  # ‚úÖ correct name
}


# ==========================================
# ‚öôÔ∏è HELPERS
# ==========================================
def make_row(instruction, response, input_ctx=""):
    return {
        "instruction": instruction.strip(),
        "input": input_ctx.strip(),
        "response": response.strip(),
    }


def safe_extract_conv(example):
    instr = next(
        (
            example[k]
            for k in ["instruction", "prompt", "text"]
            if k in example and example[k]
        ),
        "",
    )
    resp = next(
        (
            example[k]
            for k in ["output", "response", "target"]
            if k in example and example[k]
        ),
        "",
    )
    inp = ""
    if "input" in example and example["input"] and str(example["input"]).strip():
        inp = str(example["input"]).strip()
    return {
        "instruction": str(instr).strip(),
        "input": inp,
        "response": str(resp).strip(),
    }


def is_valid_row(example):
    return bool(example["instruction"]) and bool(example["response"])


def map_wiki(example):
    return make_row(
        instruction=f"Provide a detailed explanation about {example['title']}.",
        response=str(example["text"])[:6000],
    )


# ==========================================
# üîß PATCH 1 ‚Äî Andhra Pradesh conv
# ==========================================
print("\n" + "=" * 60)
print("üîß PATCH 1: Andhra Pradesh ‚Äî fixing conv split")
print("=" * 60)

try:
    conv_ds = load_dataset(ANDHRA_CONV_FIX["conv"], split="train")
    mapped = conv_ds.map(safe_extract_conv, remove_columns=conv_ds.column_names)
    conv_split = mapped.filter(is_valid_row)
    print(f"  ‚úÖ Conv: {len(conv_split)} rows from {ANDHRA_CONV_FIX['conv']}")

    # Load existing splits from hub and add/replace conv
    try:
        existing = DatasetDict.load_from_hub(HF_REPO_NAME, config_name="Andhra_Pradesh")
        existing["conv"] = conv_split
        patched = existing
    except Exception:
        # No existing dataset yet ‚Äî push conv alone
        patched = DatasetDict({"conv": conv_split})

    patched.push_to_hub(HF_REPO_NAME, config_name="Andhra_Pradesh")
    print("  üéâ Andhra_Pradesh conv patched and uploaded!")

except Exception as e:
    print(f"  ‚ö†Ô∏è Andhra Pradesh conv patch failed: {e}")

# ==========================================
# üîß PATCH 2 ‚Äî Reload wiki_qa for NE fallback states
# ==========================================
ne_states = [s for s, c in WIKIQA_PATCH.items() if c["cult_val"] is None]
if ne_states:
    print("\n‚è≥ Loading microsoft/wiki_qa for NE state fallbacks...")
    global_wikiqa = load_dataset("microsoft/wiki_qa", split="train").filter(
        lambda x: x["label"] == 1
    )
    print(f"‚úÖ WikiQA: {len(global_wikiqa)} answered QA pairs")

# ==========================================
# üîß PATCH 2 ‚Äî All wikiqa cult splits
# ==========================================
for state, patch in WIKIQA_PATCH.items():
    print(f"\n{'=' * 60}")
    print(f"üîß PATCH 2 cult: {state}")
    print(f"{'=' * 60}")

    cult_split = None

    # --- States with a native Wikipedia dump ---
    if patch["cult_val"] is not None:
        try:
            print(f"  Loading Wikipedia: {patch['cult_val']}")
            wiki_ds = load_dataset(
                "wikimedia/wikipedia", patch["cult_val"], split="train"
            )

            # Try native-script keywords first
            if patch["cult_keywords"]:
                filtered = wiki_ds.filter(
                    lambda x, kw=patch["cult_keywords"]: any(
                        k in x["title"] for k in kw
                    )
                )
                print(f"  Native keyword filter ‚Üí {len(filtered)} articles")
            else:
                filtered = wiki_ds.filter(lambda x: False)  # empty

            # Fallback to romanized keywords if native got too few
            if len(filtered) < 10:
                print(
                    f"  ‚ö†Ô∏è  Too few native matches ‚Äî trying romanized fallback keywords..."
                )
                filtered = wiki_ds.filter(
                    lambda x, kw=patch["cult_fallback"]: any(
                        k.lower() in x["title"].lower() for k in kw
                    )
                )
                print(f"  Romanized fallback ‚Üí {len(filtered)} articles")

            if len(filtered) > 0:
                cult_split = filtered.map(
                    map_wiki, remove_columns=filtered.column_names
                )
                print(f"  ‚úÖ Cult (wiki_filtered): {len(cult_split)} articles")
            else:
                print(
                    f"  ‚ö†Ô∏è  Still 0 articles after both filters ‚Äî skipping cult for {state}"
                )

        except Exception as e:
            print(f"  ‚ö†Ô∏è  Wiki load error for {state}: {e}")

    # --- NE states: fall back to microsoft/wiki_qa with column-level filter ---
    else:
        try:
            keywords = patch["cult_fallback"]
            # ‚úÖ Filter on specific columns ‚Äî NOT str(entire_row)
            filtered_qa = global_wikiqa.filter(
                lambda x, kw=keywords: any(
                    k.lower() in (x["question"] + " " + x["document_title"]).lower()
                    for k in kw
                )
            )
            print(f"  WikiQA column filter ‚Üí {len(filtered_qa)} rows")

            if len(filtered_qa) > 0:

                def map_wikiqa(example, s=state):
                    return make_row(
                        instruction=(
                            f"Explain this aspect of {s.replace('_', ' ')} in detail: "
                            f"{example['question']}"
                        ),
                        response=example["answer"],
                        input_ctx=example["document_title"],
                    )

                cult_split = filtered_qa.map(
                    map_wikiqa, remove_columns=filtered_qa.column_names
                )
                print(f"  ‚úÖ Cult (wiki_qa fallback): {len(cult_split)} QA rows")
            else:
                print(
                    f"  ‚ö†Ô∏è  0 WikiQA rows for {state} ‚Äî no cult split will be uploaded"
                )

        except Exception as e:
            print(f"  ‚ö†Ô∏è  WikiQA filter error for {state}: {e}")

    # --- Push cult split ---
    if cult_split is not None and len(cult_split) > 0:
        try:
            existing = DatasetDict.load_from_hub(HF_REPO_NAME, config_name=state)
            existing["cult"] = cult_split
            patched = existing
        except Exception:
            patched = DatasetDict({"cult": cult_split})

        patched.push_to_hub(HF_REPO_NAME, config_name=state)
        print(f"  üéâ {state} cult split uploaded! ({len(cult_split)} rows)")
    else:
        print(f"  ‚ùå Skipped upload for {state} ‚Äî empty cult split.")

print("\n" + "=" * 60)
print("üöÄ ALL PATCHES APPLIED SUCCESSFULLY!")
print("=" * 60)
